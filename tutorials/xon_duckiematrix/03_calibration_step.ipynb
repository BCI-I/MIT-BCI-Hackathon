{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration\n",
    "In this notebook we will see how to design, implement and train a machine learning model to provide inputs to the Duckiebot using EEG data. \n",
    "We will use code very similar to the one you wrote in the previous notebook to stream and filter the EEG data and to extract the features. \n",
    "\n",
    "\n",
    "The calibration learns the mapping between the input space (EEG) and\n",
    "the (rotation, speed) action space for the duckie. The action space is\n",
    "2-dimensional and is represented by a rectangular visual display. To calibrate,\n",
    "a red ball travels via a serpentine pattern in this display. The user should\n",
    "simultaneously track the ball. After completing it's tour, a model is trained\n",
    "and saved, and the process repeats. The model trains after each trial, and the\n",
    "model-decoded ball is displayed as a green ball. These trials will loop (and the\n",
    "calibration will improve) indefinitely, so terminate the program whenever you\n",
    "think it is good enough. When you terminate, it may take a couple seconds to\n",
    "navigate to the terminal to do so, during which your EEG input will not be good\n",
    "for calibration, so when you want to terminate we recommend focusing on the\n",
    "calibration through the end of a trial, waiting for the next trial to start\n",
    "(indicating training for the previous trial has completed), then terminate\n",
    "before the new trial ends.\n",
    "\n",
    "In practice, for the duckie controller in duckie.py, only the rotation component\n",
    "of the action space is used. So to calibrate you can ignore the vertical\n",
    "(height) axis and just focus on left/right. Clenching your jaw on the left and\n",
    "right side to track the left/right position of the red ball leads to pretty good\n",
    "calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gui as gui_lib\n",
    "from pathlib import Path\n",
    "\n",
    "_SNAPSHOT_DIR = Path(__file__).parent / 'snapshots'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MLP\n",
    "The model we will use is a simple Multi Layer Perceptron (MLP), here we show how to implement it using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    \"\"\"MLP model.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, layer_features, activation=None):\n",
    "        \"\"\"Constructor.\n",
    "        \n",
    "        Args:\n",
    "            in_features: Int. Number of features for input to MLP.\n",
    "            layer_featuers: Iterable of ints. Number of features for each layer\n",
    "                after input.\n",
    "            activation: None or torch activation function. Defaults to\n",
    "                torch.nn.Sigmoid().\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self._in_features = in_features\n",
    "        self._layer_features = layer_features\n",
    "        if activation is None:\n",
    "            activation = torch.nn.Sigmoid()\n",
    "        self.activation = activation\n",
    "\n",
    "        features_list = [in_features] + list(layer_features)\n",
    "        module_list = []\n",
    "        for i in range(len(features_list) - 1):\n",
    "            if i > 0:\n",
    "                module_list.append(activation)\n",
    "            layer = torch.nn.Linear(\n",
    "                in_features=features_list[i],\n",
    "                out_features=features_list[i + 1]\n",
    "            )\n",
    "            module_list.append(layer)\n",
    "        \n",
    "        self.net = torch.nn.Sequential(*module_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    @property\n",
    "    def in_features(self):\n",
    "        return self._in_features\n",
    "\n",
    "    @property\n",
    "    def layer_features(self):\n",
    "        return self._layer_features\n",
    "        \n",
    "    @property\n",
    "    def out_features(self):\n",
    "        return self._layer_features[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Agent\n",
    "\n",
    "Next, we want to define an `Agent` class which is responsible for mapping the features extracted from the EEG data into actions,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class Agent(torch.nn.Module):\n",
    "    \"\"\"Agent class.\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 name,\n",
    "                 in_features,\n",
    "                 hidden_layer_sizes=(256, 256),\n",
    "                 out_features=2,\n",
    "                 snapshot_name=None):\n",
    "        \"\"\"Constructor.\n",
    "        \n",
    "        Args:\n",
    "            in_features: Int. Number of features of input to agent. In practice\n",
    "                this is the number of features of the EEG feature extractor.\n",
    "            out_features: Int Number of features for the output action space.\n",
    "            hidden_layer_sizes: Iterable of ints. Number of features for hidden\n",
    "                layers of MLP.\n",
    "            snapshot_name: None or string. If string, must name a snapshot in\n",
    "                ./snapshots/ directory, in which case the model parameters are\n",
    "                initialized from that snapshot. Otherwise randomly initialized\n",
    "                model parameters.\n",
    "        \"\"\"\n",
    "        super(Agent, self).__init__()\n",
    "        self._name = name\n",
    "        self._snapshot_path = _SNAPSHOT_DIR / name\n",
    "        \n",
    "        self._net = MLP(\n",
    "            in_features=in_features,\n",
    "            layer_features=tuple(list(hidden_layer_sizes) + [out_features]),\n",
    "        )\n",
    "        if snapshot_name is not None:\n",
    "            # Load agent from snapshot\n",
    "            state_dict_path = _SNAPSHOT_DIR / snapshot_name\n",
    "            self._net.load_state_dict(torch.load(state_dict_path))\n",
    "            print(f'Loaded from snapshot {state_dict_path}')\n",
    "            \n",
    "    def __call__(self, features, as_numpy=True):\n",
    "        \"\"\"Convert features to action.\n",
    "        \n",
    "        Input features if a numpy array and may be either batched of size\n",
    "        [batch_size, in_features] or un-batched of size [in_features].\n",
    "        \"\"\"\n",
    "        no_batch = len(features.shape) == 1\n",
    "        \n",
    "        if no_batch:\n",
    "            features = features[None]\n",
    "        action = self._net(torch.from_numpy(features.astype(np.float32)))\n",
    "        if no_batch:\n",
    "            action = action[0]\n",
    "            \n",
    "        # Convert to numpy is necessary\n",
    "        if as_numpy:\n",
    "            action = action.detach().numpy()\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def snapshot(self):\n",
    "        \"\"\"Save model parameters.\"\"\"\n",
    "        torch.save(self._net.state_dict(), self._snapshot_path)\n",
    "        print(f'Saved snapshot to {self._snapshot_path}')\n",
    "\n",
    "\n",
    "def _sample_batch(*arrays, batch_size):\n",
    "    \"\"\"Sample a batch of data from arrays.\"\"\"\n",
    "    num_samples = arrays[0].shape[0]\n",
    "    indices = np.random.choice(num_samples, size=batch_size)\n",
    "    batch_arrays = [x[indices] for x in arrays]\n",
    "    return batch_arrays\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The calibrator. \n",
    "\n",
    "We're ready to define the calibrator class to run the calibration process.\n",
    "\n",
    "Note that the at the end of the calibration we will save a `snapshot` of the MLP model (it's weights). You can fine tune your model through subsequent calibration steps by loading this snapshot and continuing the training.\n",
    "\n",
    "Once you're happy with the performance of your model and you can control the calibration space well, you can move on to the next notebook to implement the duckie controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "class Calibrator():\n",
    "    \"\"\"Action space calibrator to learn mapping from EEG features to actions.\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 name,\n",
    "                 feature_stream,\n",
    "                 gui,\n",
    "                 snapshot_name=None,\n",
    "                 batch_size=128,\n",
    "                 training_steps=2000,\n",
    "                 optimizer=torch.optim.SGD,\n",
    "                 lr=0.01,\n",
    "                 grad_clip=1):\n",
    "        \"\"\"Constructor.\n",
    "        \n",
    "        Args:\n",
    "            name: String. Name for this calibration. The model will be saved to\n",
    "                ./snapshots/$name after each trial through the gui.\n",
    "            features_stream: Callable that returns current features. See\n",
    "                lsl_api.py.\n",
    "            gui: TKinter gui for the calibration. See gui.py.\n",
    "            snapshot_name: None or string. If string, must point to a snapshot\n",
    "                in ./snapshots/ directory and model parameters will be\n",
    "                initialized from that snapshot. If None, model parameters are\n",
    "                randomly initialized.\n",
    "            batch_size: Int. Batch size for model training.\n",
    "            training_steps: Int. Number of model training steps to perform after\n",
    "                each calibration trial.\n",
    "            optimized: Torch optimizer for training.\n",
    "            lr: Float. Learning rate for optimizer.\n",
    "            grad_clip: Scalar. Gradient clipping.\n",
    "        \"\"\"\n",
    "        self._feature_stream = feature_stream\n",
    "        self._gui = gui\n",
    "        self._name = name\n",
    "        self._agent = Agent(\n",
    "            in_features=feature_stream.n_features,\n",
    "            out_features=gui.n_features,\n",
    "            name=name,\n",
    "            snapshot_name=snapshot_name,\n",
    "        )\n",
    "        \n",
    "        # Optimization\n",
    "        self._batch_size = batch_size\n",
    "        self._training_steps = training_steps\n",
    "        self._optimizer = optimizer(self._agent.parameters(), lr=lr)\n",
    "        self._grad_clip = grad_clip\n",
    "        \n",
    "    def _train(self):\n",
    "        \"\"\"Run training loop and save model.\"\"\"\n",
    "        print(f'Training')\n",
    "        all_inputs = np.array(self._all_inputs)\n",
    "        all_targets = np.array(self._all_targets)\n",
    "        for _ in range(self._training_steps):\n",
    "            self._optimizer.zero_grad()\n",
    "            \n",
    "            # Sample batch\n",
    "            batch_inputs, batch_targets = _sample_batch(\n",
    "                all_inputs, all_targets, batch_size=self._batch_size,\n",
    "            )\n",
    "            batch_outputs = self._agent(batch_inputs, as_numpy=False)\n",
    "            batch_targets = torch.from_numpy(batch_targets.astype(np.float32))\n",
    "            \n",
    "            # Evaluate loss\n",
    "            loss = torch.mean(torch.sum(torch.square(\n",
    "                batch_targets - batch_outputs), axis=1))\n",
    "            \n",
    "            # Backprop\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self._agent.parameters(), self._grad_clip)\n",
    "            self._optimizer.step()\n",
    "            \n",
    "        # Save model\n",
    "        print(f'Saving')\n",
    "        self._agent.snapshot()\n",
    "    \n",
    "    def __call__(self):\n",
    "        \"\"\"Run calibration loop.\"\"\"\n",
    "        self._gui.reset()\n",
    "        self._all_inputs = []\n",
    "        self._all_targets = []\n",
    "        \n",
    "        # Create callback function for the gui\n",
    "        def _callback(target, fin):\n",
    "            if fin:\n",
    "                print('Finished calibration trial')\n",
    "                self._train()\n",
    "                self._gui.reset()\n",
    "                self._all_inputs = []\n",
    "                self._all_targets = []\n",
    "                \n",
    "            features = self._feature_stream()\n",
    "            agent_pos = self._agent(features)\n",
    "            self._all_inputs.append(features)\n",
    "            self._all_targets.append(target)\n",
    "            return agent_pos\n",
    "\n",
    "        # Set gui callback and run main loop\n",
    "        self._gui.set_callback(_callback)\n",
    "        self._gui.root.after(3, self._gui.step)\n",
    "        self._gui.root.mainloop()\n",
    "        \n",
    "        # Save inputs and targets for training\n",
    "        self._all_inputs = np.array(self._all_inputs)\n",
    "        self._all_targets = np.array(self._all_targets)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the calibration.\n",
    "\n",
    "Now that the code is ready, we can run the calibration process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gui and calibrator \n",
    "from lsl_apy_complete import CompleteStreamer\n",
    "\n",
    "pilot_name: str = \"YourName\"\n",
    "STREAM_NAME = \"X.on-102106-0035\"  # Replace with your stream name\n",
    "\n",
    "feature_stream = CompleteStreamer(STREAM_NAME)\n",
    "\n",
    "gui = gui_lib.CalibrationGUI()\n",
    "calibrator = Calibrator(\n",
    "    name=pilot_name,\n",
    "    feature_stream=feature_stream,\n",
    "    gui=gui,\n",
    "    # snapshot_name=snapshot_name,  # use this to fine tune a previous snapshot\n",
    ")\n",
    "\n",
    "# Run calibration loop\n",
    "calibrator()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
